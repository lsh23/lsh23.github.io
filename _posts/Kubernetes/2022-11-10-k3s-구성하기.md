---
title: k3s 구성하기
date: 2022-11-10 22:00:00 +0900
categories: [Kubernetes]
tags: [Kubernetes, k3s]
comments: true
---

kubernets 실습 및 스터디를 위한 간단한 k3s 구성하기


## k3s란
k3s는 비교적 설치가 간단하고  
IoT, edge computing 디바이스위에서 동작하도록 만들어진 경량 쿠버네티스이다.


## 구성
OS : ubuntu 22.04  
1개의 master 노드와 2개의 worker 노드로 구성

## 사전 준비
k3s 공식 docs에 cluster 구성을 위해 필요한 networking에 대해 나열되어 있다.  
설치할 노드에 맞게 iptables를 통해서 규칙을 추가해준다.

| Protocol | Port | Source | Description |
|:---------|:-----|:-------|:------------|
| TCP | 6443 | K3s agent nodes | Kubernetes API Server |
| TCP | 10250 | K3s server and agent nodes | Kubelet metrics |

master node
```console
$ sudo iptables -F # 초기화
$ sudo iptables -I INPUT -p tcp --dport 6443 -j ACCEPT
$ sudo iptables -I INPUT -p tcp --dport 10250 -j ACCEPT
$ sudo iptables -I INPUT -p tcp --dport 30000:32767 -j ACCEPT # node port
```
worker node
```console
$ sudo iptables -F # 초기화
$ sudo iptables -I INPUT -p tcp --dport 10250 -j ACCEPT
$ sudo iptables -I INPUT -p tcp --dport 30000:32767 -j ACCEPT # node port
```

> 사용하는 리소스가 cloud 리소스일 경우 보안그룹에도 규칙을 넣어줘야한다.
{: .prompt-tip }

## maser 노드 설치
아래의 명령으로 설치
```console
$ curl -sfL https://get.k3s.io | sh -
```
설치 후에 worker 노드 설치에 필요한 TOKEN 값 확인
```console
$ cat /var/lib/rancher/k3s/server/node-token
K10373314897d684c2bbd8b01ff147449fa35acaafc9a99e549909f7bcea514070b::server:2dc963b5a5b61b77f94418c811f059f2
```

## worker 노드 설치
```console
$ MASTER_IP=xxx.xxx.xxx.xxx
$ TOKEN=K10373314897d684c2bbd8b01ff147449fa35acaafc9a99e549909f7bcea514070b::server:2dc963b5a5b61b77f94418c811f059f2
$ curl -sfL https://get.k3s.io | K3S_URL=https://$MASTER_IP:6443 K3S_TOKEN=$TOKEN sh -
```
> TOKEN은 master 노드에서 확인한 값을 넣어준다.
{: .prompt-info }

## 설치 확인 on master
```console
ubuntu@master:~$ sudo kubectl cluster-info
Kubernetes control plane is running at https://127.0.0.1:6443
CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```
```console
ubuntu@master:~$ sudo kubectl get node
NAME      STATUS   ROLES                  AGE   VERSION
master    Ready    control-plane,master   36m   v1.25.3+k3s1
worker2   Ready    <none>                 28m   v1.25.3+k3s1
worker1   Ready    <none>                 28m   v1.25.3+k3s1
```

## 원격 kubectl 
master 노드의 /etc/rancher/k3s/k3s.yaml 파일을 복사해서 '~/.kube/k3s-config' 파일을 생성한다.  
server의 ip는 master 노드의 ip로 수정한다.  
name은 클러스터 구분자라고 생각하면된다. default로 되어있는데, 자신이 원하는 이름으로 수정하면된다.

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: ...
    server: https://127.0.0.1:6443 <- 해당 ip를 master 노드의 ip로 수정
  name: default <- 이름 수정
contexts:
- context: 
    cluster: default <- 이름 수정
    user: default <- 이름 수정
  name: default <- 이름 수정
current-context: default <- 이름 수정
kind: Config
preferences: {}
users:
- name: default <- 이름 수정
  user:
    client-certificate-data: ...
    client-key-data: ...

```
> default라는 이름으로 다른 클러스터를 관리하고 있다면 반드시 이름을 변경해야한다.
{: .prompt-warning }
> vi 편집기에서 아래 명령어를 통해서 일괄 변경이 쉽게 가능하다.   
> `:%s/127.0.0.1/master-ip/g`  
> `:%s/default/your-k3s-name/g`
{: .prompt-tip }

환경변수인 KUBECONFIG에 위에서 생성한 k3s-config 파일의 경로를 추가해준다.
```shell
export KUBECONFIG=$HOME/.kube/k3s-config:$HOME/.kube/config:$KUBECONFIG
```
{: file='~/.bashrc'}
> ~/.bashrc , ~/.zshrc 등 사용하는 shell의 프로파일에 해당 내용 추가
{: .prompt-tip }


```console
solver@solver:~$ kubectl config use-context your-k3s-name
solver@solver:~$ kubectl get node -o wide
NAME      STATUS   ROLES                  AGE   VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
master    Ready    control-plane,master   87m   v1.25.4+k3s1   10.0.0.167    <none>        Ubuntu 22.04.1 LTS   5.15.0-1021-oracle   containerd://1.6.8-k3s1
worker1   Ready    <none>                 86m   v1.25.4+k3s1   10.0.0.47     <none>        Ubuntu 22.04.1 LTS   5.15.0-1021-oracle   containerd://1.6.8-k3s1
worker2   Ready    <none>                 86m   v1.25.4+k3s1   10.0.0.237    <none>        Ubuntu 22.04.1 LTS   5.15.0-1021-oracle   containerd://1.6.8-k3s1
```

## 원격 kubectl시 ssl 오류가 난다면
cloud 리소스로 k3s를 구축했다면 아래와 같은 에러가 발생한다.
```console
solver@solver:~$ kubectl get pod
Unable to connect to the server: x509: certificate is valid for 10.0.0.167, 10.43.0.1, 127.0.0.1, ::1, not xxx.xxx.xxx.xxx
```

에러가 나는 이유는 k3s가 구성 될때 Kubernetes API 서버의 인증서가 내부 IP로만 발행되서 생기는 문제다.
아래 목록에서 공인 ip에 대한 정보는 없다.
```console
ubuntu@master:~$ sudo kubectl -n kube-system get secrets/k3s-serving -o yaml 
apiVersion: v1
data:
  tls.crt: ...
  tls.key: ...
kind: Secret
metadata:
  annotations:
    listener.cattle.io/cn-__1-f16284: ::1
    listener.cattle.io/cn-10.0.0.167: 10.0.0.167
    listener.cattle.io/cn-10.43.0.1: 10.43.0.1
    listener.cattle.io/cn-127.0.0.1: 127.0.0.1
    listener.cattle.io/cn-kubernetes: kubernetes
    listener.cattle.io/cn-kubernetes.default: kubernetes.default
    listener.cattle.io/cn-kubernetes.default.svc: kubernetes.default.svc
    listener.cattle.io/cn-kubernetes.default.svc.cluster.local: kubernetes.default.svc.cluster.local
    listener.cattle.io/cn-localhost: localhost
    listener.cattle.io/cn-master: master
    ...
```


master 노드에서 아래의 절차를 진행하면 해결 할 수 있다.

/etc/systemd/system/k3s.service 파일에 k3s server가 시작될때 --tls-san 옵션을 가지고 실행되도록 수정해준다.
값은 master 노드의 public ip를 넣어준다.
```
ExecStart=/usr/local/bin/k3s \
    server --tls-san xxx.xxx.xxx.xxx \
```
{: file='/etc/systemd/system/k3s.service'}


기존의 k3s-serving secrets을 삭제 후 dynamic-cert.json 파일을 삭제(또는 백업) 후 k3s를 재시작하면 새로 secrets을 생성한다.
```
kubectl -n kube-system delete secrets/k3s-serving
mv /var/lib/rancher/k3s/server/tls/dynamic-cert.json /var/lib/rancher/k3s/server/tls/dynamic-cert.json.bak

systemctl systemctl daemon-reload
systemctl restart k3s
```

재시작 후 공인 ip 정보가 들어갔는지 확인한다.
```console
ubuntu@master:~$ sudo kubectl -n kube-system get secrets/k3s-serving -o yaml 
apiVersion: v1
data:
  tls.crt: ...
  tls.key: ...
kind: Secret
metadata:
  annotations:
    listener.cattle.io/cn-__1-f16284: ::1
    listener.cattle.io/cn-10.0.0.167: 10.0.0.167
    listener.cattle.io/cn-10.43.0.1: 10.43.0.1
    listener.cattle.io/cn-127.0.0.1: 127.0.0.1
    listener.cattle.io/cn-xxx.xxx.xxx.xxx: xxx.xxx.xxx.xxx   <--- 들어갔다면 성공!
    listener.cattle.io/cn-kubernetes: kubernetes
    listener.cattle.io/cn-kubernetes.default: kubernetes.default
    listener.cattle.io/cn-kubernetes.default.svc: kubernetes.default.svc
    listener.cattle.io/cn-kubernetes.default.svc.cluster.local: kubernetes.default.svc.cluster.local
    listener.cattle.io/cn-localhost: localhost
    listener.cattle.io/cn-master: master
    ...
```

다시 로컬 환경에서 kubectl이 동작하는지 확인한다.
```console
solver@solver:~$ kubectl get node -o wide
NAME      STATUS   ROLES                  AGE   VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
master    Ready    control-plane,master   87m   v1.25.4+k3s1   10.0.0.167    <none>        Ubuntu 22.04.1 LTS   5.15.0-1021-oracle   containerd://1.6.8-k3s1
worker1   Ready    <none>                 86m   v1.25.4+k3s1   10.0.0.47     <none>        Ubuntu 22.04.1 LTS   5.15.0-1021-oracle   containerd://1.6.8-k3s1
worker2   Ready    <none>                 86m   v1.25.4+k3s1   10.0.0.237    <none>        Ubuntu 22.04.1 LTS   5.15.0-1021-oracle   containerd://1.6.8-k3s1
```


## reference
* <https://docs.k3s.io/>
* <https://si.mpli.st/dev/2020-01-01-easy-k8s-with-k3s/>
* <https://cigiko.cafe24.com/ubuntu-20-04-%EC%84%9C%EB%B2%84%EC%97%90-k3s-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-%EA%B5%AC%EC%84%B1%ED%95%98%EA%B8%B0/>
* <https://github.com/k3s-io/k3s/issues/535/>
* <https://github.com/k3s-io/k3s/issues/1381/>



